import streamlit as st
from bedrock_client import invoke_claude
from bedrock_agent_client import retrieve_and_generate_with_kb, sync_knowledge_base
from lambda_client import invoke_lambda_function
import os

# Show title and description.
st.title("ðŸ’¬ Chatbot")
st.write(
    "This is a simple chatbot powered by AWS Bedrock's Claude model. "
    "You can ask questions and receive answers generated by the model. "
    "Optionally, you can enhance responses with retrieval-augmented generation from specific knowledge bases or datasources. "
)

st.markdown("---")

# Create a session state variable to store the chat messages. This ensures that the
# messages persist across reruns.
if "messages" not in st.session_state:
    st.session_state.messages = []

# Password protection
# if "authenticated" not in st.session_state:
#     st.session_state["authenticated"] = False

# if not st.session_state["authenticated"]:
#     password = st.text_input("Enter password", type="password")
    # if password:
    #     if password == os.environ.get("APP_PASSWORD"):
    #         st.session_state["authenticated"] = True
#             st.rerun()
#         else:
#             st.error("Incorrect password. Please try again.")
#     st.stop()

st.markdown("---")

# Knowledge base selection in sidebar
with st.sidebar:
    st.subheader("Knowledge Base (Optional)")
    st.write("Select a knowledge base to enhance your chat with domain-specific knowledge, or leave blank to use the general Claude model.")
    kb_map = {"None": "", "Astrology": "6YC60AQVKG"}  # Add more as needed
    kb_names = list(kb_map.keys())
    kb_selected_name = st.selectbox("Knowledge Base:", kb_names, index=0)
    kb_selected_id = kb_map[kb_selected_name]

    # Change this later
    dataSourceId = "2EETF2VIUC"
    
    # Sync button for knowledge base
    if kb_selected_id:
        st.markdown("---")
        st.subheader("Knowledge Base Sync")
        st.write("Sync the knowledge base to ensure it's up to date with the latest data sources.")
        if st.button("ðŸ”„ Sync Knowledge Base", type="secondary"):
            with st.spinner("Syncing knowledge base..."):
                sync_result = sync_knowledge_base(kb_selected_id, dataSourceId)
                if sync_result:
                    st.success("Knowledge base sync started successfully! This may take a few minutes to complete.")
                    st.info(f"Job ID: {sync_result.get('ingestionJob', {}).get('ingestionJobId', 'N/A')}")
                else:
                    st.error("Failed to start knowledge base sync. Please try again.")
    
    st.markdown("---")
    
    st.subheader("PDF Processing")
    st.write("Enter a PDF URL to process and add to the knowledge base.")
    pdf_url = st.text_input("PDF URL:", placeholder="https://example.com/document.pdf")
    if st.button("ðŸ“„ Process PDF", type="secondary"):
        if pdf_url:
            with st.spinner("Processing PDF..."):
                parameters = {"url": pdf_url}

                lambda_result = invoke_lambda_function(parameters)
                if lambda_result:
                    # Check if the lambda response indicates completion
                    if isinstance(lambda_result, dict) and lambda_result.get('statusCode') == 200:
                        st.success("PDF processing completed successfully!")
                    elif isinstance(lambda_result, dict) and lambda_result.get('statusCode') == 202:
                        st.success("PDF processing started successfully!")
                    else:
                        st.success("PDF processing initiated!")
                    
                    # Extract user-friendly message from response
                    user_message = "PDF processing completed"
                    if isinstance(lambda_result, dict):
                        response_body = lambda_result.get('response', {}).get('functionResponse', {}).get('responseBody', {})
                        text_body = response_body.get('TEXT', {}).get('body', '')
                        if text_body:
                            user_message = text_body
                    
                    # st.info(f"Message: {user_message}")
                    st.info(f"Result: {lambda_result}")
                else:
                    st.error("Failed to process PDF. Please try again.")
        else:
            st.warning("Please enter a PDF URL.")
    
    st.markdown("---")
    
    st.subheader("Model Selection")
    st.write("Choose the AI model for generating responses.")
    inference_profiles = ["us.anthropic.claude-3-7-sonnet-20250219-v1:0", 
                          "us.anthropic.claude-3-5-haiku-20241022-v1:0"
                          ]  # Add more as needed
    inference_profile_id = st.selectbox("Model:", inference_profiles, index=0)

st.markdown("---")

# Display the existing chat messages via `st.chat_message`.
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Create a chat input field to allow the user to enter a message. This will display
# automatically at the bottom of the page.
if prompt := st.chat_input("What is up?"):

    # Store and display the current prompt.
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Spinner while generating response
    with st.spinner("Assistant is thinking..."):
        if kb_selected_id:
            response = retrieve_and_generate_with_kb(st.session_state.messages, knowledge_base_id=kb_selected_id, inference_profile_id=inference_profile_id)
        else:
            response = invoke_claude(st.session_state.messages, inference_profile_id=inference_profile_id)

    # Display and store the response.
    with st.chat_message("assistant"):
        st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})
